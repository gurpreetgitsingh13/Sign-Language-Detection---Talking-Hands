# ğŸ§  Talking Hands â€“ Real-Time Sign Language Detection âœ‹ğŸ¤Ÿ

Welcome to **Talking Hands**, a real-time sign language detection system built to interpret American Sign Language (ASL) gestures using computer vision and deep learning.

## ğŸš€ Project Overview

Talking Hands uses webcam video to detect hand gestures and classify them into ASL numbers (0â€“9). The goal is to bridge communication gaps and make gesture-based interactions more accessible using AI.

## ğŸ¯ Key Features

- ğŸ” Real-time hand gesture recognition
- ğŸ¤– CNN-based model trained on ASL digit images
- ğŸ§  MediaPipe & OpenCV for hand tracking and image preprocessing
- ğŸ’» Webcam-based live predictions
- ğŸ“Š Visual output with gesture overlay and predicted digit

## ğŸ› ï¸ Tech Stack

| Tool/Library   | Purpose                           |
|----------------|-----------------------------------|
| Python         | Main programming language         |
| OpenCV         | Video capture and preprocessing   |
| MediaPipe      | Hand tracking and landmarks       |
| TensorFlow/Keras | Model training and inference    |
| NumPy/Pandas   | Data handling and manipulation    |
| Matplotlib     | Model evaluation and visualization|

## ğŸ–¼ï¸ Model Architecture

- Convolutional Neural Network (CNN) trained on ASL digit dataset (0â€“9)
- Input: Processed hand images from webcam
- Output: Digit class (0â€“9)

## ğŸ“ Project Structure

```
Talking-Hands/
â”œâ”€â”€ data/                  # ASL digit dataset
â”œâ”€â”€ model/                 # Trained model files
â”œâ”€â”€ src/                   # Python source code
â”‚   â”œâ”€â”€ hand_tracker.py
â”‚   â”œâ”€â”€ predict.py
â”‚   â””â”€â”€ train_model.py
â”œâ”€â”€ test_images/           # Sample test images
â”œâ”€â”€ README.md              # This file
â””â”€â”€ requirements.txt       # Python dependencies
```

## ğŸ§ª How to Run

1. **Clone the repo**
   ```bash
   git clone https://github.com/gurpreetgitsingh13/Sign-Language-Detection---Talking-Hands.git
   cd Sign-Language-Detection---Talking-Hands
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Run the detection script**
   ```bash
   python src/predict.py
   ```

4. **Make hand gestures (0â€“9) in front of your webcam and see predictions in real-time!**

## ğŸ“ˆ Future Improvements

- ğŸ”¤ Support for full ASL alphabet
- ğŸŒ Web interface using Flask or React
- ğŸ“± Mobile deployment (Android/iOS)
- ğŸ—£ï¸ Gesture-to-speech integration

## ğŸ¤ Contributing

Pull requests are welcome! For major changes, please open an issue first to discuss what you would like to change.

## ğŸ“„ License

MIT License Â© Gurpreet Singh

## ğŸ™Œ Acknowledgements

- [ASL Dataset](https://www.kaggle.com/datasets/datamunge/sign-language-mnist)
- [MediaPipe by Google](https://mediapipe.dev/)
- TensorFlow + Keras community

---

> _â€œCode is poetry. Data is the voice of reality.â€_
